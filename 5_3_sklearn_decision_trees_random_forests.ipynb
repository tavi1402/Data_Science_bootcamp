{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tavi1402/Data_Science_bootcamp/blob/main/5_3_sklearn_decision_trees_random_forests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "expensive-trainer",
      "metadata": {
        "id": "expensive-trainer"
      },
      "source": [
        "# Decision Trees and Random Forests\n",
        "\n",
        "![](https://i.imgur.com/N8aIuRK.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "indonesian-montreal",
      "metadata": {
        "id": "indonesian-montreal"
      },
      "source": [
        "The following topics are covered in this tutorial:\n",
        "\n",
        "- Downloading a real-world dataset\n",
        "- Preparing a dataset for training\n",
        "- Training and interpreting decision trees\n",
        "- Training and interpreting random forests\n",
        "- Overfitting & hyperparameter tuning\n",
        "- Making predictions on single inputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cheap-emission",
      "metadata": {
        "id": "cheap-emission"
      },
      "source": [
        "### How to run the code\n",
        "\n",
        "This tutorial is an executable [Jupyter notebook](https://jupyter.org) hosted on [Jovian](https://www.jovian.ai). You can _run_ this tutorial and experiment with the code examples in a couple of ways: *using free online resources* (recommended) or *on your computer*.\n",
        "\n",
        "#### Option 1: Running using free online resources (1-click, recommended)\n",
        "\n",
        "The easiest way to start executing the code is to click the **Run** button at the top of this page and select **Run on Colab**. You will be prompted to connect your Google Drive account so that this notebook can be placed into your drive for execution.\n",
        "\n",
        "\n",
        "#### Option 2: Running on your computer locally\n",
        "\n",
        "To run the code on your computer locally, you'll need to set up [Python](https://www.python.org), download the notebook and install the required libraries. We recommend using the [Conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/) distribution of Python. Click the **Run** button at the top of this page, select the **Run Locally** option, and follow the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "charged-english",
      "metadata": {
        "id": "charged-english"
      },
      "source": [
        "## Problem Statement\n",
        "\n",
        "This tutorial takes a practical and coding-focused approach. We'll learn how to use _decision trees_ and _random forests_ to solve a real-world problem from [Kaggle](https://kaggle.com/datasets):\n",
        "\n",
        "> **QUESTION**: The [Rain in Australia dataset](https://kaggle.com/jsphyg/weather-dataset-rattle-package) contains about 10 years of daily weather observations from numerous Australian weather stations. Here's a small sample from the dataset:\n",
        ">\n",
        "> ![](https://i.imgur.com/5QNJvir.png)\n",
        ">\n",
        "> As a data scientist at the Bureau of Meteorology, you are tasked with creating a fully-automated system that can use today's weather data for a given location to predict whether it will rain at the location tomorrow.\n",
        ">\n",
        ">\n",
        "> ![](https://i.imgur.com/KWfcpcO.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "statutory-mayor",
      "metadata": {
        "id": "statutory-mayor"
      },
      "source": [
        "Let's install and import some required libraries before we begin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "smaller-moore",
      "metadata": {
        "id": "smaller-moore"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wraQ_3GLEOwt",
      "metadata": {
        "id": "wraQ_3GLEOwt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "plain-explorer",
      "metadata": {
        "id": "plain-explorer"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "fitting-grace",
      "metadata": {
        "id": "fitting-grace"
      },
      "source": [
        "## Downloading the Data\n",
        "\n",
        "The dataset is available at https://www.kaggle.com/jsphyg/weather-dataset-rattle-package .\n",
        "\n",
        "\n",
        "We'll use the [`opendatasets` library](https://github.com/JovianML/opendatasets) to download the data from Kaggle directly within Jupyter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "brave-prerequisite",
      "metadata": {
        "id": "brave-prerequisite"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "funded-compression",
      "metadata": {
        "id": "funded-compression"
      },
      "source": [
        "The dataset is downloaded and extracted to the folder `weather-dataset-rattle-package`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "realistic-search",
      "metadata": {
        "id": "realistic-search"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "divided-papua",
      "metadata": {
        "id": "divided-papua"
      },
      "source": [
        "The file `weatherAUS.csv` contains the data. Let's load it into a Pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "established-smart",
      "metadata": {
        "id": "established-smart"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "requested-occurrence",
      "metadata": {
        "id": "requested-occurrence"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "enabling-indianapolis",
      "metadata": {
        "id": "enabling-indianapolis"
      },
      "source": [
        "Each row shows the measurements for a given date at a given location. The last column \"RainTomorrow\" contains the value to be predicted.\n",
        "\n",
        "Let's check the column types of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deadly-murray",
      "metadata": {
        "id": "deadly-murray"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "handy-recipient",
      "metadata": {
        "id": "handy-recipient"
      },
      "source": [
        "\n",
        "\n",
        "Let's drop any rows where the value of the target column `RainTomorrow` in empty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "seven-franchise",
      "metadata": {
        "id": "seven-franchise"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "honest-sleeve",
      "metadata": {
        "id": "honest-sleeve"
      },
      "source": [
        "Let's save our work before continuing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "proper-chinese",
      "metadata": {
        "id": "proper-chinese"
      },
      "source": [
        "> **EXERCISE**: Perform exploratory data analysis on the dataset and study the relationship of other columns with the `RainTomorrow` column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "running-cloud",
      "metadata": {
        "id": "running-cloud"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "varying-traveler",
      "metadata": {
        "id": "varying-traveler"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "peaceful-season",
      "metadata": {
        "id": "peaceful-season"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "adapted-project",
      "metadata": {
        "id": "adapted-project"
      },
      "source": [
        "## Preparing the Data for Training\n",
        "\n",
        "We'll perform the following steps to prepare the dataset for training:\n",
        "\n",
        "1. Create a train/test/validation split\n",
        "2. Identify input and target columns\n",
        "3. Identify numeric and categorical columns\n",
        "4. Impute (fill) missing numeric values\n",
        "5. Scale numeric values to the $(0, 1)$ range\n",
        "6. Encode categorical columns to one-hot vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bizarre-period",
      "metadata": {
        "id": "bizarre-period"
      },
      "source": [
        "### Training, Validation and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "secure-dominican",
      "metadata": {
        "id": "secure-dominican"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "medium-prospect",
      "metadata": {
        "id": "medium-prospect"
      },
      "source": [
        "While working with chronological data, it's often a good idea to separate the training, validation and test sets with time, so that the model is trained on data from the past and evaluated on data from the future.\n",
        "\n",
        "We'll use the data till 2014 for the training set, data from 2015 for the validation set, and the data from 2016 & 2017 for the test set.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "quarterly-pillow",
      "metadata": {
        "id": "quarterly-pillow"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "convenient-evidence",
      "metadata": {
        "id": "convenient-evidence"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "intellectual-nature",
      "metadata": {
        "id": "intellectual-nature"
      },
      "source": [
        "### Input and Target Columns\n",
        "\n",
        "Let's identify the input and target columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "musical-skating",
      "metadata": {
        "id": "musical-skating"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "recreational-console",
      "metadata": {
        "id": "recreational-console"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "parliamentary-painting",
      "metadata": {
        "id": "parliamentary-painting"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "superior-dance",
      "metadata": {
        "id": "superior-dance"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "static-temperature",
      "metadata": {
        "id": "static-temperature"
      },
      "source": [
        "Let's also identify the numeric and categorical columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "welcome-viking",
      "metadata": {
        "id": "welcome-viking"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "starting-teach",
      "metadata": {
        "id": "starting-teach"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bacterial-proportion",
      "metadata": {
        "id": "bacterial-proportion"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "fancy-county",
      "metadata": {
        "id": "fancy-county"
      },
      "source": [
        "> **EXERCISE**: Study how various columns are correlated with the target and select just a subset of the columns, instead of all of the. Observe how it affects the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "reliable-portable",
      "metadata": {
        "id": "reliable-portable"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "induced-tuner",
      "metadata": {
        "id": "induced-tuner"
      },
      "source": [
        "### Imputing missing numeric values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "metric-storm",
      "metadata": {
        "id": "metric-storm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cardiac-nelson",
      "metadata": {
        "id": "cardiac-nelson"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pacific-peter",
      "metadata": {
        "id": "pacific-peter"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "assisted-brisbane",
      "metadata": {
        "id": "assisted-brisbane"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "incomplete-librarian",
      "metadata": {
        "id": "incomplete-librarian"
      },
      "source": [
        "> **EXERCISE**: Try a different [imputation strategy](https://scikit-learn.org/stable/modules/impute.html#impute) and observe how it affects the results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "concerned-sight",
      "metadata": {
        "id": "concerned-sight"
      },
      "source": [
        "### Scaling Numeric Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sonic-conclusion",
      "metadata": {
        "id": "sonic-conclusion"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "premium-brother",
      "metadata": {
        "id": "premium-brother"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "brief-raising",
      "metadata": {
        "id": "brief-raising"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "heard-librarian",
      "metadata": {
        "id": "heard-librarian"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "opened-membership",
      "metadata": {
        "id": "opened-membership"
      },
      "source": [
        "> **EXERCISE**: Try a different [scaling strategy](https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling) and observe how it affects the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sublime-louisville",
      "metadata": {
        "id": "sublime-louisville"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "complicated-engineering",
      "metadata": {
        "id": "complicated-engineering"
      },
      "source": [
        "### Encoding Categorical Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "extra-gauge",
      "metadata": {
        "id": "extra-gauge"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "armed-nursery",
      "metadata": {
        "id": "armed-nursery"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "balanced-gamma",
      "metadata": {
        "id": "balanced-gamma"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rapid-circulation",
      "metadata": {
        "id": "rapid-circulation"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "overhead-functionality",
      "metadata": {
        "id": "overhead-functionality"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "swiss-emergency",
      "metadata": {
        "id": "swiss-emergency"
      },
      "source": [
        "> **EXERCISE**: Try a different [encoding strategy](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features) and observe how it affects the results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sexual-actor",
      "metadata": {
        "id": "sexual-actor"
      },
      "source": [
        "As a final step, let's drop the textual categorical columns, so that we're left with just numeric data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "floating-cathedral",
      "metadata": {
        "id": "floating-cathedral"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "standard-brooklyn",
      "metadata": {
        "id": "standard-brooklyn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "bound-balance",
      "metadata": {
        "id": "bound-balance"
      },
      "source": [
        "Let's save our work before continuing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "informed-studio",
      "metadata": {
        "id": "informed-studio"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "handy-robinson",
      "metadata": {
        "id": "handy-robinson"
      },
      "source": [
        "## Training and Visualizing Decision Trees\n",
        "\n",
        "A decision tree in general parlance represents a hierarchical series of binary decisions:\n",
        "\n",
        "<img src=\"https://i.imgur.com/qSH4lqz.png\" width=\"480\">\n",
        "\n",
        "A decision tree in machine learning works in exactly the same way, and except that we let the computer figure out the optimal structure & hierarchy of decisions, instead of coming up with criteria manually."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "exempt-retail",
      "metadata": {
        "id": "exempt-retail"
      },
      "source": [
        "### Training\n",
        "\n",
        "We can use `DecisionTreeClassifier` from `sklearn.tree` to train a decision tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dress-reviewer",
      "metadata": {
        "id": "dress-reviewer"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "activated-designer",
      "metadata": {
        "id": "activated-designer"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "superb-fight",
      "metadata": {
        "id": "superb-fight"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "available-wheat",
      "metadata": {
        "id": "available-wheat"
      },
      "source": [
        "An optimal decision tree has now been created using the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "industrial-valuation",
      "metadata": {
        "id": "industrial-valuation"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Let's evaluate the decision tree using the accuracy score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ongoing-broadway",
      "metadata": {
        "id": "ongoing-broadway"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "religious-personality",
      "metadata": {
        "id": "religious-personality"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "invisible-medicaid",
      "metadata": {
        "id": "invisible-medicaid"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "demographic-guyana",
      "metadata": {
        "id": "demographic-guyana"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "naked-luxembourg",
      "metadata": {
        "id": "naked-luxembourg"
      },
      "source": [
        "The decision tree also returns probabilities for each prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "equipped-violin",
      "metadata": {
        "id": "equipped-violin"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "involved-affair",
      "metadata": {
        "id": "involved-affair"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "handled-finnish",
      "metadata": {
        "id": "handled-finnish"
      },
      "source": [
        "Seems like the decision tree is quite confident about its predictions.\n",
        "\n",
        "Let's check the accuracy of its predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "psychological-wellington",
      "metadata": {
        "id": "psychological-wellington"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "certified-employee",
      "metadata": {
        "id": "certified-employee"
      },
      "source": [
        "The training set accuracy is close to 100%! But we can't rely solely on the training set accuracy, we must evaluate the model on the validation set too.\n",
        "\n",
        "We can make predictions and compute accuracy in one step using `model.score`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "logical-values",
      "metadata": {
        "id": "logical-values"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ruled-today",
      "metadata": {
        "id": "ruled-today"
      },
      "source": [
        "Although the training accuracy is 100%, the accuracy on the validation set is just about 79%, which is only marginally better then always predicting \"No\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "original-national",
      "metadata": {
        "id": "original-national"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "bibliographic-divide",
      "metadata": {
        "id": "bibliographic-divide"
      },
      "source": [
        "It appears that the model has learned the training examples perfect, and doesn't generalize well to previously unseen examples. This phenomenon is called \"overfitting\", and reducing overfitting is one of the most important parts of any machine learning project."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "promotional-third",
      "metadata": {
        "id": "promotional-third"
      },
      "source": [
        "### Visualization\n",
        "\n",
        "We can visualize the decision tree _learned_ from the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "universal-catalog",
      "metadata": {
        "id": "universal-catalog"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "existing-bangkok",
      "metadata": {
        "id": "existing-bangkok"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "external-wisconsin",
      "metadata": {
        "id": "external-wisconsin"
      },
      "source": [
        "Can you see how the model classifies a given input as a series of decisions? The tree is truncated here, but following any path from the root node down to a leaf will result in \"Yes\" or \"No\". Do you see how a decision tree differs from a logistic regression model?\n",
        "\n",
        "\n",
        "**How a Decision Tree is Created**\n",
        "\n",
        "Note the `gini` value in each box. This is the loss function used by the decision tree to decide which column should be used for splitting the data, and at what point the column should be split. A lower Gini index indicates a better split. A perfect split (only one class on each side) has a Gini index of 0.\n",
        "\n",
        "For a mathematical discussion of the Gini Index, watch this video: https://www.youtube.com/watch?v=-W0DnxQK1Eo . It has the following formula:\n",
        "\n",
        "<img src=\"https://i.imgur.com/CSC0gAo.png\" width=\"240\">\n",
        "\n",
        "Conceptually speaking, while training the models evaluates all possible splits across all possible columns and picks the best one. Then, it recursively performs an optimal split for the two portions. In practice, however, it's very inefficient to check all possible splits, so the model uses a heuristic (predefined strategy) combined with some randomization.\n",
        "\n",
        "The iterative approach of the machine learning workflow in the case of a decision tree involves growing the tree layer-by-layer:\n",
        "\n",
        "<img src=\"https://i.imgur.com/tlYiXnp.png\" width=\"480\">\n",
        "\n",
        "\n",
        "Let's check the depth of the tree that was created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bronze-split",
      "metadata": {
        "id": "bronze-split"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "sharing-provision",
      "metadata": {
        "id": "sharing-provision"
      },
      "source": [
        "We can also display the tree as text, which can be easier to follow for deeper trees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "congressional-circle",
      "metadata": {
        "id": "congressional-circle",
        "scrolled": false
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "opponent-butler",
      "metadata": {
        "id": "opponent-butler"
      },
      "source": [
        "> **EXERCISE**: Based on the above discussion, can you explain why the training accuracy is 100% whereas the validation accuracy is lower?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vocal-arthur",
      "metadata": {
        "id": "vocal-arthur"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "talented-watson",
      "metadata": {
        "id": "talented-watson"
      },
      "source": [
        "### Feature Importance\n",
        "\n",
        "Based on the gini index computations, a decision tree assigns an \"importance\" value to each feature. These values can be used to interpret the results given by a decision tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ambient-commons",
      "metadata": {
        "id": "ambient-commons"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "fatal-frost",
      "metadata": {
        "id": "fatal-frost"
      },
      "source": [
        "Let's turn this into a dataframe and visualize the most important features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "meaning-stake",
      "metadata": {
        "id": "meaning-stake"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "unique-symphony",
      "metadata": {
        "id": "unique-symphony"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fatty-biotechnology",
      "metadata": {
        "id": "fatty-biotechnology"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "scheduled-insurance",
      "metadata": {
        "id": "scheduled-insurance"
      },
      "source": [
        "Let's save our work before continuing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "instrumental-drive",
      "metadata": {
        "id": "instrumental-drive"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "composed-airfare",
      "metadata": {
        "id": "composed-airfare"
      },
      "source": [
        "## Hyperparameter Tuning and Overfitting\n",
        "\n",
        "As we saw in the previous section, our decision tree classifier memorized all training examples, leading to a 100% training accuracy, while the validation accuracy was only marginally better than a dumb baseline model. This phenomenon is called overfitting, and in this section, we'll look at some strategies for reducing overfitting.\n",
        "\n",
        "\n",
        "The `DecisionTreeClassifier` accepts several arguments, some of which can be modified to reduce overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "distinguished-trainer",
      "metadata": {
        "id": "distinguished-trainer"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "derived-attribute",
      "metadata": {
        "id": "derived-attribute"
      },
      "source": [
        "These arguments are called hyperparameters because they must be configured manually (as opposed to the parameters within the model which are _learned_ from the data. We'll explore a couple of hyperparameters:\n",
        "\n",
        "- `max_depth`\n",
        "- `max_leaf_nodes`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "solved-cocktail",
      "metadata": {
        "id": "solved-cocktail"
      },
      "source": [
        "### `max_depth`\n",
        "\n",
        "By reducing the maximum depth of the decision tree, we can prevent the tree from memorizing all training examples, which may lead to better generalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "specialized-appearance",
      "metadata": {
        "id": "specialized-appearance"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "professional-consultancy",
      "metadata": {
        "id": "professional-consultancy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "radio-blair",
      "metadata": {
        "id": "radio-blair"
      },
      "source": [
        "We can compute the accuracy of the model on the training and validation sets using `model.score`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "applicable-retirement",
      "metadata": {
        "id": "applicable-retirement"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "detected-temperature",
      "metadata": {
        "id": "detected-temperature"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "delayed-street",
      "metadata": {
        "id": "delayed-street"
      },
      "source": [
        "Great, while the training accuracy of the model has gone down, the validation accuracy of the model has increased significantly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "legendary-commodity",
      "metadata": {
        "id": "legendary-commodity"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "accessory-ireland",
      "metadata": {
        "id": "accessory-ireland"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "accurate-tackle",
      "metadata": {
        "id": "accurate-tackle"
      },
      "source": [
        "> **EXERCISE**: Study the decision tree diagram carefully and understand what each of the terms `gini`, `samples`, `value` and `class` mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "coral-payment",
      "metadata": {
        "id": "coral-payment"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "suspected-climb",
      "metadata": {
        "id": "suspected-climb"
      },
      "source": [
        "Let's experiment with different depths using a helper function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tutorial-coating",
      "metadata": {
        "id": "tutorial-coating"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "elegant-nothing",
      "metadata": {
        "id": "elegant-nothing"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "answering-climate",
      "metadata": {
        "id": "answering-climate"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "expanded-external",
      "metadata": {
        "id": "expanded-external"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "present-click",
      "metadata": {
        "id": "present-click"
      },
      "source": [
        "This is a common pattern you'll see with all machine learning algorithms:\n",
        "\n",
        "<img src=\"https://i.imgur.com/EJCrSZw.png\" width=\"480\">\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "radical-victory",
      "metadata": {
        "id": "radical-victory"
      },
      "source": [
        "You'll often need to tune hyperparameters carefully to find the optimal fit. In the above case, it appears that a maximum depth of 7 results in the lowest validation error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "biblical-tourist",
      "metadata": {
        "id": "biblical-tourist"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "thirty-witness",
      "metadata": {
        "id": "thirty-witness"
      },
      "source": [
        "### `max_leaf_nodes`\n",
        "\n",
        "Another way to control the size of complexity of a decision tree is to limit the number of leaf nodes. This allows branches of the tree to have varying depths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "patent-durham",
      "metadata": {
        "id": "patent-durham"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gorgeous-clothing",
      "metadata": {
        "id": "gorgeous-clothing"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "clinical-hands",
      "metadata": {
        "id": "clinical-hands"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "digital-belgium",
      "metadata": {
        "id": "digital-belgium"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "corporate-glenn",
      "metadata": {
        "id": "corporate-glenn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "brilliant-religious",
      "metadata": {
        "id": "brilliant-religious"
      },
      "source": [
        "Notice that the model was able to achieve a greater depth of 12 for certain paths while keeping other paths shorter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "grand-interface",
      "metadata": {
        "id": "grand-interface"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "northern-security",
      "metadata": {
        "id": "northern-security"
      },
      "source": [
        "> **EXERCISE**: Find the combination of `max_depth` and `max_leaf_nodes` that results in the highest validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "revised-guard",
      "metadata": {
        "id": "revised-guard"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "purple-chaos",
      "metadata": {
        "id": "purple-chaos"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ambient-appendix",
      "metadata": {
        "id": "ambient-appendix"
      },
      "source": [
        "> **EXERCISE**: Explore and experiment with other arguments of `DecisionTree`. Refer to the docs for details: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cardiac-nashville",
      "metadata": {
        "id": "cardiac-nashville"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aerial-holiday",
      "metadata": {
        "id": "aerial-holiday"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "coral-amendment",
      "metadata": {
        "id": "coral-amendment"
      },
      "source": [
        "> **EXERCISE**: A more advanced technique (but less commonly used technique) for reducing overfitting in decision trees is known as cost-complexity pruning. Learn more about it here: https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html . Implement cost complexity pruning. Do you see any improvement in the validation accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "understood-crown",
      "metadata": {
        "id": "understood-crown"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vertical-bacteria",
      "metadata": {
        "id": "vertical-bacteria"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "sophisticated-conjunction",
      "metadata": {
        "id": "sophisticated-conjunction"
      },
      "source": [
        "Let's save our work before continuing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "celtic-norwegian",
      "metadata": {
        "id": "celtic-norwegian"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "bibliographic-correlation",
      "metadata": {
        "id": "bibliographic-correlation"
      },
      "source": [
        "## Training a Random Forest\n",
        "\n",
        "While tuning the hyperparameters of a single decision tree may lead to some improvements, a much more effective strategy is to combine the results of several decision trees trained with slightly different parameters. This is called a random forest model.\n",
        "\n",
        "The key idea here is that each decision tree in the forest will make different kinds of errors, and upon averaging, many of their errors will cancel out. This idea is also commonly known as the \"wisdom of the crowd\":\n",
        "\n",
        "<img src=\"https://i.imgur.com/4Dg0XK4.png\" width=\"480\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "included-monte",
      "metadata": {
        "id": "included-monte"
      },
      "source": [
        "A random forest works by averaging/combining the results of several decision trees:\n",
        "\n",
        "<img src=\"https://1.bp.blogspot.com/-Ax59WK4DE8w/YK6o9bt_9jI/AAAAAAAAEQA/9KbBf9cdL6kOFkJnU39aUn4m8ydThPenwCLcBGAsYHQ/s0/Random%2BForest%2B03.gif\" width=\"640\">\n",
        "\n",
        "\n",
        "We'll use the `RandomForestClassifier` class from `sklearn.ensemble`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "employed-arrest",
      "metadata": {
        "id": "employed-arrest"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sufficient-stick",
      "metadata": {
        "id": "sufficient-stick"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "durable-kitchen",
      "metadata": {
        "id": "durable-kitchen"
      },
      "source": [
        "`n_jobs` allows the random forest to use mutiple parallel workers to train decision trees, and `random_state=42` ensures that the we get the same results for each execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "alpha-stable",
      "metadata": {
        "id": "alpha-stable"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "advanced-evidence",
      "metadata": {
        "id": "advanced-evidence"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "optimum-flash",
      "metadata": {
        "id": "optimum-flash"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "democratic-pilot",
      "metadata": {
        "id": "democratic-pilot"
      },
      "source": [
        "Once again, the training accuracy is almost 100%, but this time the validation accuracy is much better. In fact, it is better than the best single decision tree we had trained so far. Do you see the power of random forests?\n",
        "\n",
        "This general technique of combining the results of many models is called \"ensembling\", it works because most errors of individual models cancel out on averaging. Here's what it looks like visually:\n",
        "\n",
        "<img src=\"https://i.imgur.com/qJo8D8b.png\" width=\"640\">\n",
        "\n",
        "\n",
        "We can also look at the probabilities for the predictions. The probability of a class is simply the fraction of trees which that predicted the given class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "major-aquatic",
      "metadata": {
        "id": "major-aquatic"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "handled-analysis",
      "metadata": {
        "id": "handled-analysis"
      },
      "source": [
        "We can can access individual decision trees using `model.estimators_`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "harmful-pizza",
      "metadata": {
        "id": "harmful-pizza"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "naked-receipt",
      "metadata": {
        "id": "naked-receipt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "secret-mason",
      "metadata": {
        "id": "secret-mason"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "valid-palestine",
      "metadata": {
        "id": "valid-palestine"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "legislative-storage",
      "metadata": {
        "id": "legislative-storage"
      },
      "source": [
        "> **EXERCISE**: Verify that none of the individual decision trees have a better validation accuracy than the random forest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imposed-correlation",
      "metadata": {
        "id": "imposed-correlation"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "satisfactory-scope",
      "metadata": {
        "id": "satisfactory-scope"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "electoral-lounge",
      "metadata": {
        "id": "electoral-lounge"
      },
      "source": [
        "Just like decision tree, random forests also assign an \"importance\" to each feature, by combining the importance values from individual trees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "urban-neutral",
      "metadata": {
        "id": "urban-neutral"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "outer-position",
      "metadata": {
        "id": "outer-position"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "perfect-alias",
      "metadata": {
        "id": "perfect-alias"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "inappropriate-tackle",
      "metadata": {
        "id": "inappropriate-tackle"
      },
      "source": [
        "Notice that the distribution is a lot less skewed than that for a single decision tree."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adjacent-jaguar",
      "metadata": {
        "id": "adjacent-jaguar"
      },
      "source": [
        "Let's save our work before continuing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "backed-lebanon",
      "metadata": {
        "id": "backed-lebanon"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "residential-ireland",
      "metadata": {
        "id": "residential-ireland"
      },
      "source": [
        "## Hyperparameter Tuning with Random Forests\n",
        "\n",
        "Just like decision trees, random forests also have several hyperparameters. In fact many of these hyperparameters are applied to the underlying decision trees.\n",
        "\n",
        "Let's study some the hyperparameters for random forests. You can learn more about them here: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dominican-evans",
      "metadata": {
        "id": "dominican-evans"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "sound-delhi",
      "metadata": {
        "id": "sound-delhi"
      },
      "source": [
        "Let's create a base model with which we can compare models with tuned hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nasty-officer",
      "metadata": {
        "id": "nasty-officer"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bound-sheep",
      "metadata": {
        "id": "bound-sheep"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hidden-squad",
      "metadata": {
        "id": "hidden-squad"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "better-cliff",
      "metadata": {
        "id": "better-cliff"
      },
      "source": [
        "We can use this as a benchmark for hyperparmeter tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cellular-belfast",
      "metadata": {
        "id": "cellular-belfast"
      },
      "source": [
        "### `n_estimators`\n",
        "\n",
        "This argument controls the number of decision trees in the random forest. The default value is 100. For larger datasets, it helps to have a greater number of estimators. As a general rule, try to have as few estimators as needed.\n",
        "\n",
        "\n",
        "**10 estimators**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "complicated-roller",
      "metadata": {
        "id": "complicated-roller"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "spanish-familiar",
      "metadata": {
        "id": "spanish-familiar",
        "scrolled": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "illegal-turkish",
      "metadata": {
        "id": "illegal-turkish"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "catholic-alabama",
      "metadata": {
        "id": "catholic-alabama"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "soviet-canon",
      "metadata": {
        "id": "soviet-canon"
      },
      "source": [
        "**500 estimators**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "narrative-prayer",
      "metadata": {
        "id": "narrative-prayer"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "legislative-export",
      "metadata": {
        "id": "legislative-export"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "impressive-guide",
      "metadata": {
        "id": "impressive-guide"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tribal-storm",
      "metadata": {
        "id": "tribal-storm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "german-afghanistan",
      "metadata": {
        "id": "german-afghanistan"
      },
      "source": [
        "> **EXERCISE**: Vary the value of `n_estimators` and plot the graph between training error and validation error. What is the optimal value of `n_estimators`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "injured-strengthening",
      "metadata": {
        "id": "injured-strengthening"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "temporal-kenya",
      "metadata": {
        "id": "temporal-kenya"
      },
      "source": [
        "### `max_depth` and `max_leaf_nodes`\n",
        "\n",
        "These arguments are passed directly to each decision tree, and control the maximum depth and max. no leaf nodes of each tree respectively. By default, no maximum depth is specified, which is why each tree has a training accuracy of 100%. You can specify a `max_depth` to reduce overfitting.\n",
        "\n",
        "<img src=\"https://i.imgur.com/EJCrSZw.png\" width=\"480\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "norwegian-little",
      "metadata": {
        "id": "norwegian-little"
      },
      "source": [
        "Let's define a helper function `test_params` to make it easy to test hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "forbidden-society",
      "metadata": {
        "id": "forbidden-society"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "local-machinery",
      "metadata": {
        "id": "local-machinery"
      },
      "source": [
        "Let's test a few values of `max_depth` and `max_leaf_nodes`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "handled-sentence",
      "metadata": {
        "id": "handled-sentence"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "addressed-virtue",
      "metadata": {
        "id": "addressed-virtue"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "geological-portsmouth",
      "metadata": {
        "id": "geological-portsmouth"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "functioning-virtue",
      "metadata": {
        "id": "functioning-virtue"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "renewable-berkeley",
      "metadata": {
        "id": "renewable-berkeley"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "breeding-audience",
      "metadata": {
        "id": "breeding-audience"
      },
      "source": [
        "The optimal values of `max_depth` and `max_leaf_nodes` lies somewhere between 0 and unbounded."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "smooth-result",
      "metadata": {
        "id": "smooth-result"
      },
      "source": [
        "> **EXERCISE**: Vary the value of `max_depth` and plot the graph between training error and validation error. What is the optimal value of `max_depth`? Do the same for `max_leaf_nodes`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "united-wilson",
      "metadata": {
        "id": "united-wilson"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rental-substance",
      "metadata": {
        "id": "rental-substance"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "nuclear-asbestos",
      "metadata": {
        "id": "nuclear-asbestos"
      },
      "source": [
        "### `max_features`\n",
        "\n",
        "Instead of picking all features (columns) for every split, we can specify that only a fraction of features be chosen randomly to figure out a split.\n",
        "\n",
        "<img src=\"https://i.imgur.com/FXGWMDY.png\" width=\"720\">\n",
        "\n",
        "Notice that the default value `auto` causes only $\\sqrt{n}$ out of total features ( $n$ ) to be chosen randomly at each split. This is the reason each decision tree in the forest is different. While it may seem counterintuitive, choosing all features for every split of every tree will lead to identical trees, so the random forest will not generalize well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "victorian-assurance",
      "metadata": {
        "id": "victorian-assurance"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "invisible-complement",
      "metadata": {
        "id": "invisible-complement"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sonic-davis",
      "metadata": {
        "id": "sonic-davis"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "typical-delivery",
      "metadata": {
        "id": "typical-delivery"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "difficult-clinic",
      "metadata": {
        "id": "difficult-clinic"
      },
      "source": [
        "> **EXERCISE**: Find the optimal values of `max_features` for this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "billion-publication",
      "metadata": {
        "id": "billion-publication"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "korean-vessel",
      "metadata": {
        "id": "korean-vessel"
      },
      "source": [
        "### `min_samples_split` and `min_samples_leaf`\n",
        "\n",
        "By default, the decision tree classifier tries to split every node that has 2 or more. You can increase the values of these arguments to change this behavior and reduce overfitting, especially for very large datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "latter-teddy",
      "metadata": {
        "id": "latter-teddy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "conventional-abuse",
      "metadata": {
        "id": "conventional-abuse"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "earned-particle",
      "metadata": {
        "id": "earned-particle"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "funky-labor",
      "metadata": {
        "id": "funky-labor"
      },
      "source": [
        "> **EXERCISE**: Find the optimal values of `min_samples_split` and `min_samples_leaf`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "least-physics",
      "metadata": {
        "id": "least-physics"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "prepared-evaluation",
      "metadata": {
        "id": "prepared-evaluation"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "duplicate-specialist",
      "metadata": {
        "id": "duplicate-specialist"
      },
      "source": [
        "### `min_impurity_decrease`\n",
        "\n",
        "This argument is used to control the threshold for splitting nodes. A node will be split if this split induces a decrease of the impurity (Gini index) greater than or equal to this value. It's default value is 0, and you can increase it to reduce overfitting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "flexible-surprise",
      "metadata": {
        "id": "flexible-surprise"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "parental-northeast",
      "metadata": {
        "id": "parental-northeast"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "provincial-number",
      "metadata": {
        "id": "provincial-number"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "immune-attachment",
      "metadata": {
        "id": "immune-attachment"
      },
      "source": [
        "> **EXERCISE**: Find the optimal values of `min_impurity_decrease` for this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fatal-stranger",
      "metadata": {
        "id": "fatal-stranger"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "express-accident",
      "metadata": {
        "id": "express-accident"
      },
      "source": [
        "### `bootstrap`, `max_samples`\n",
        "\n",
        "By default, a random forest doesn't use the entire dataset for training each decision tree. Instead it applies a technique called bootstrapping. For each tree, rows from the dataset are picked one by one randomly, with replacement i.e. some rows may not show up at all, while some rows may show up multiple times.\n",
        "\n",
        "\n",
        "<img src=\"https://i.imgur.com/W8UGaEA.png\" width=\"640\">\n",
        "\n",
        "Bootstrapping helps the random forest generalize better, because each decision tree only sees a fraction of th training set, and some rows randomly get higher weightage than others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dirty-biodiversity",
      "metadata": {
        "id": "dirty-biodiversity"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hollow-martin",
      "metadata": {
        "id": "hollow-martin"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "intellectual-preservation",
      "metadata": {
        "id": "intellectual-preservation"
      },
      "source": [
        "When bootstrapping is enabled, you can also control the number or fraction of rows to be considered for each bootstrap using `max_samples`. This can further generalize the model.\n",
        "\n",
        "<img src=\"https://i.imgur.com/rsdrL1W.png\" width=\"640\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "commercial-nightmare",
      "metadata": {
        "id": "commercial-nightmare"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "altered-boulder",
      "metadata": {
        "id": "altered-boulder"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "relative-manner",
      "metadata": {
        "id": "relative-manner"
      },
      "source": [
        "Learn more about bootstrapping here: https://towardsdatascience.com/what-is-out-of-bag-oob-score-in-random-forest-a7fa23d710"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "industrial-mistress",
      "metadata": {
        "id": "industrial-mistress"
      },
      "source": [
        "### `class_weight`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cordless-arena",
      "metadata": {
        "id": "cordless-arena"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dominant-element",
      "metadata": {
        "id": "dominant-element"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "known-northern",
      "metadata": {
        "id": "known-northern"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "interpreted-situation",
      "metadata": {
        "id": "interpreted-situation"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "floppy-gospel",
      "metadata": {
        "id": "floppy-gospel"
      },
      "source": [
        "> **EXERCISE**: Find the optimal value of `class_weight` for this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "weird-window",
      "metadata": {
        "id": "weird-window"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "piano-equality",
      "metadata": {
        "id": "piano-equality"
      },
      "source": [
        "### Putting it together\n",
        "\n",
        "Let's train a random forest with customized hyperparameters based on our learnings. Of course, different hyperpraams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "elder-income",
      "metadata": {
        "id": "elder-income"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "brutal-huntington",
      "metadata": {
        "id": "brutal-huntington"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "worth-commerce",
      "metadata": {
        "id": "worth-commerce"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "finished-difference",
      "metadata": {
        "id": "finished-difference"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "tracked-demographic",
      "metadata": {
        "id": "tracked-demographic"
      },
      "source": [
        "We've increased the accuracy from 84.5% with a single decision tree to 85.7% with a well-tuned random forest. Depending on the dataset and the kind of problem, you may or may not a see a significant improvement with hyperparameter tuning.\n",
        "\n",
        "This could be due to any of the following reasons:\n",
        "\n",
        "- We may not have found the right mix of hyperparameters to regularize (reduce overfitting) the model properly, and we should keep trying to improve the model.\n",
        "\n",
        "- We may have reached the limits of the modeling technique we're currently using (Random Forests), and we should try another modeling technique e.g. gradient boosting.\n",
        "\n",
        "- We may have reached the limits of what we can predict using the given amount of data, and we may need more data to improve the model.\n",
        "\n",
        "- We may have reached the limits of how well we can predict whether it will rain tomorrow using the given weather measurements, and we may need more features (columns) to further improve the model. In many cases, we can also generate new features using existing features (this is called feature engineering).\n",
        "\n",
        "- Whether it will rain tomorrow may be an inherently random or chaotic phenomenon which simply cannot be predicted beyond a certain accuracy any amount of data for any number of weather measurements with any modeling technique.  \n",
        "\n",
        "Remember that ultimately all models are wrong, but some are useful. If you can rely on the model we've created today to make a travel decision for tomorrow, then the model is useful, even though it may sometimes be wrong."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SZavKPwjpz2O",
      "metadata": {
        "id": "SZavKPwjpz2O"
      },
      "source": [
        "### Strategy for Tuning Hyperparameters\n",
        "\n",
        "Here's a good strategy for tuning hyperparameters:\n",
        "\n",
        "1. Tune the most important/impactful hyperparameter first e.g. n_estimators\n",
        "\n",
        "2. With the best value of the first hyperparameter, tune the next most impactful hyperparameter\n",
        "\n",
        "3. And so on, keep training the next most impactful parameters with the best values for previous parameters...\n",
        "\n",
        "4. Then, go back to the top and further tune each parameter again for further marginal gains\n",
        "\n",
        "Keep your ideas and experiments organized using an experiment tracking sheet: https://bit.ly/mltrackingsheet\n",
        "\n",
        "\n",
        "Your first objective should be make the training loss as low as possible (even if the validation loss is very large), and then try to regularize the model to slowly decrease the valiadation loss while increasing the training loss.\n",
        "\n",
        "\n",
        "Hyperparameter tuning is more art than science, unfortunately. Try to get a feel for how the parameters interact with each other based on your understanding of the parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "radio-speed",
      "metadata": {
        "id": "radio-speed"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "numerical-prague",
      "metadata": {
        "id": "numerical-prague"
      },
      "source": [
        "> **EXERCISE**: Experiment with the hyperparameters of the random forest classifier, and try to maximize the validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "brazilian-mauritius",
      "metadata": {
        "id": "brazilian-mauritius"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "turned-oriental",
      "metadata": {
        "id": "turned-oriental"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "considerable-royalty",
      "metadata": {
        "id": "considerable-royalty"
      },
      "source": [
        "Finally, let's also compute the accuracy of our model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "serial-button",
      "metadata": {
        "id": "serial-button"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "accompanied-least",
      "metadata": {
        "id": "accompanied-least"
      },
      "source": [
        "Notice that the test accuracy is lower"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wound-decline",
      "metadata": {
        "id": "wound-decline"
      },
      "source": [
        "Let's save our work before continuing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "improved-synthesis",
      "metadata": {
        "id": "improved-synthesis"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "faced-church",
      "metadata": {
        "id": "faced-church"
      },
      "source": [
        "## Making Predictions on New Inputs\n",
        "\n",
        "Let's define a helper function to make predictions on new inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "short-enhancement",
      "metadata": {
        "id": "short-enhancement"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bizarre-washington",
      "metadata": {
        "id": "bizarre-washington"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "innovative-middle",
      "metadata": {
        "id": "innovative-middle"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "usual-violation",
      "metadata": {
        "id": "usual-violation"
      },
      "source": [
        "> **EXERCISE**: Try changing the values in `new_input` and observe how the predictions and probabilities change. Try different values of location, temperature, humidity, pressure etc. Try to get an _intuitive feel_ of which columns have the greatest effect on the result of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nonprofit-trout",
      "metadata": {
        "id": "nonprofit-trout"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "otherwise-treasury",
      "metadata": {
        "id": "otherwise-treasury"
      },
      "source": [
        "## Saving and Loading Trained Models\n",
        "\n",
        "We can save the parameters (weights and biases) of our trained model to disk, so that we needn't retrain the model from scratch each time we wish to use it. Along with the model, it's also important to save imputers, scalers, encoders and even column names. Anything that will be required while generating predictions using the model should be saved.\n",
        "\n",
        "We can use the `joblib` module to save and load Python objects on the disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "legitimate-opera",
      "metadata": {
        "id": "legitimate-opera"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "competitive-windows",
      "metadata": {
        "id": "competitive-windows"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "worth-toronto",
      "metadata": {
        "id": "worth-toronto"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "monthly-mixture",
      "metadata": {
        "id": "monthly-mixture"
      },
      "source": [
        "The object can be loaded back using `joblib.load`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "opening-cream",
      "metadata": {
        "id": "opening-cream"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "scientific-receipt",
      "metadata": {
        "id": "scientific-receipt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "institutional-processor",
      "metadata": {
        "id": "institutional-processor"
      },
      "source": [
        "Let's save our work before continuing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "super-association",
      "metadata": {
        "id": "super-association"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "documented-disney",
      "metadata": {
        "id": "documented-disney"
      },
      "source": [
        "## Summary and References\n",
        "\n",
        "The following topics were covered in this tutorial:\n",
        "\n",
        "- Downloading a real-world dataset\n",
        "- Preparing a dataset for training\n",
        "- Training and interpreting decision trees\n",
        "- Training and interpreting random forests\n",
        "- Overfitting & hyperparameter tuning\n",
        "- Making predictions on single inputs\n",
        "\n",
        "\n",
        "\n",
        "We also introduced the following terms:\n",
        "\n",
        "* Decision tree\n",
        "* Random forest\n",
        "* Overfitting\n",
        "* Hyperparameter\n",
        "* Hyperparameter tuning\n",
        "* Ensembling\n",
        "* Generalization\n",
        "* Bootstrapping\n",
        "\n",
        "\n",
        "Check out the following resources to learn more:\n",
        "\n",
        "- https://scikit-learn.org/stable/modules/tree.html\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "- https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction\n",
        "- https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering\n",
        "- https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search\n",
        "- https://www.kaggle.com/c/home-credit-default-risk/discussion/64821\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acc95650",
      "metadata": {
        "id": "acc95650"
      },
      "source": [
        "## Revision Questions\n",
        "1.\tWhat is a decision tree model?\n",
        "2.\tWhat is <code>DecisionTreeClassifier()</code>?\n",
        "3.\tCan we use decision tree only for Classifier?\n",
        "4.\tHow can you visualize the decision tree?\n",
        "5.\tWhat is <code>max_depth</code> in decision tree?\n",
        "6.\tWhat is gini index?\n",
        "7.\tWhat is feature importance?\n",
        "8.\tWhat is overfitting? What could be the reason for overfitting?\n",
        "9.\tWhat is hyperparameter tuning?\n",
        "10.\tWhat is one way to control the complexity of the decision tree?\n",
        "11.\tWhat is a random forest model?\n",
        "12.\tWhat is <code>RandomForestClassifier()</code>?\n",
        "13.\tWhat is <code>model.score()</code>?\n",
        "14.\tWhat is generalization?\n",
        "15.\tWhat is ensembling?\n",
        "16.\tWhat is <code>n_estimators</code> in hyperparameter tuning of random forests?\n",
        "17.\tWhat is underfitting?\n",
        "18.\tWhat does <code>max_features</code> parameter do?\n",
        "19.\tWhat are some features that help in controlling the threshold for splitting nodes in decision tree?\n",
        "20.\tWhat is bootstrapping? What is <code>max_samples</code> parameter in bootstrapping?\n",
        "21.\tWhat is <code>class_weight</code> parameter?\n",
        "22.\tYou may or may not a see a significant improvement in the accuracy score with hyperparameter tuning. What could be the possible reasons for that?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "documented-disney"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}